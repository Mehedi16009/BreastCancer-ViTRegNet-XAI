{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNnQ3ES+5xMR5zstttNo9lB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "84379a7be43a40bb820a50e943f73bdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc2dd57ebdd546179c69fd95f046c0ac",
              "IPY_MODEL_163f112ed04c484392f4dbe604d866d5",
              "IPY_MODEL_4aef04cfb2464e42a10dff5f52fd05d4"
            ],
            "layout": "IPY_MODEL_33141610f947480e89379a9f1f436bb1"
          }
        },
        "cc2dd57ebdd546179c69fd95f046c0ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c04856d02fc3404aab8f30901c1d250b",
            "placeholder": "​",
            "style": "IPY_MODEL_d068930bfe064d8f8439f065eae62b45",
            "value": "model.safetensors: 100%"
          }
        },
        "163f112ed04c484392f4dbe604d866d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4a3c9733a38427d8cc9fde41c6b6985",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_956e2108d0764abbacbbcd2a60043cac",
            "value": 346284714
          }
        },
        "4aef04cfb2464e42a10dff5f52fd05d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dee800d9cd7d4169b4bce3c035f91cf6",
            "placeholder": "​",
            "style": "IPY_MODEL_e54e21ea9db44714820845bac37d1950",
            "value": " 346M/346M [00:01&lt;00:00, 266MB/s]"
          }
        },
        "33141610f947480e89379a9f1f436bb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c04856d02fc3404aab8f30901c1d250b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d068930bfe064d8f8439f065eae62b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f4a3c9733a38427d8cc9fde41c6b6985": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "956e2108d0764abbacbbcd2a60043cac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dee800d9cd7d4169b4bce3c035f91cf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e54e21ea9db44714820845bac37d1950": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2b1d630cb1445f6a157f3d36b9e94fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cc4d9af15da4dec96eafb3a6cacb28c",
              "IPY_MODEL_2b5d407d52b441a6941ecd2eeb464090",
              "IPY_MODEL_3a93977b62b84a9ba36d862aeaeb29e7"
            ],
            "layout": "IPY_MODEL_ecbec6ea2cec4fb295fddddea0ffbff6"
          }
        },
        "6cc4d9af15da4dec96eafb3a6cacb28c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_872380b97de34b439bb4467e71cbcadb",
            "placeholder": "​",
            "style": "IPY_MODEL_8fcc52106df142fdb7d5e173af15794a",
            "value": "model.safetensors: 100%"
          }
        },
        "2b5d407d52b441a6941ecd2eeb464090": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5af80c161b946ec9aca62438d37a1f2",
            "max": 78055620,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bc3735d20714593bf7de579263f9044",
            "value": 78055620
          }
        },
        "3a93977b62b84a9ba36d862aeaeb29e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99b76c05f83a42f5991f81e2cc2e924a",
            "placeholder": "​",
            "style": "IPY_MODEL_534d01f89e1d4ac69616e60f97e65af9",
            "value": " 78.1M/78.1M [00:00&lt;00:00, 335MB/s]"
          }
        },
        "ecbec6ea2cec4fb295fddddea0ffbff6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "872380b97de34b439bb4467e71cbcadb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fcc52106df142fdb7d5e173af15794a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5af80c161b946ec9aca62438d37a1f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bc3735d20714593bf7de579263f9044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99b76c05f83a42f5991f81e2cc2e924a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "534d01f89e1d4ac69616e60f97e65af9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mehedi16009/BreastCancer-ViTRegNet-XAI/blob/main/12th_August_Breast_Cancer_ViT%2BRegNet_50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azmeWnJWxD81",
        "outputId": "2a5cc4a5-21eb-4c75-8b6a-b72681d2adf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Install dependencies\n",
        "!pip install -q timm grad-cam albumentations opencv-python-headless scikit-learn torchmetrics\n",
        "\n",
        "# (torch & torchvision are already present in Colab typically)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: imports and seeds\n",
        "import os, random, time, math\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "import timm\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCgZAiDcxKlm",
        "outputId": "cf3e0ea6-5c53-4c5d-8b37-5e6535c4d3dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Mount Google Drive and read CSVs\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "CSV_DIR = '/content/drive/MyDrive/archive/csv'\n",
        "BASE_DIR = '/content/drive/MyDrive/archive/jpeg'   # where the jpeg/<patient_id>/*jpg files live\n",
        "\n",
        "mass_csv = os.path.join(CSV_DIR, 'mass_case_description_train_set.csv')\n",
        "calc_csv = os.path.join(CSV_DIR, 'calc_case_description_train_set.csv')\n",
        "meta_csv = os.path.join(CSV_DIR, 'meta.csv')\n",
        "\n",
        "print(\"CSV paths exist:\",\n",
        "      os.path.exists(mass_csv),\n",
        "      os.path.exists(calc_csv),\n",
        "      os.path.exists(meta_csv))\n",
        "\n",
        "mass_df = pd.read_csv(mass_csv)\n",
        "calc_df = pd.read_csv(calc_csv)\n",
        "meta_df = pd.read_csv(meta_csv)\n",
        "\n",
        "print(\"mass rows:\", len(mass_df), \"calc rows:\", len(calc_df), \"meta rows:\", len(meta_df))\n",
        "print(\"mass columns:\", mass_df.columns.tolist())\n",
        "print(\"calc columns:\", calc_df.columns.tolist())\n",
        "print(mass_df.head(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLfW7zz4xKm_",
        "outputId": "26fae326-8495-411c-a204-e981067eee3c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "CSV paths exist: True True True\n",
            "mass rows: 1318 calc rows: 1546 meta rows: 6775\n",
            "mass columns: ['patient_id', 'breast_density', 'left or right breast', 'image view', 'abnormality id', 'abnormality type', 'mass shape', 'mass margins', 'assessment', 'pathology', 'subtlety', 'image file path', 'cropped image file path', 'ROI mask file path']\n",
            "calc columns: ['patient_id', 'breast density', 'left or right breast', 'image view', 'abnormality id', 'abnormality type', 'calc type', 'calc distribution', 'assessment', 'pathology', 'subtlety', 'image file path', 'cropped image file path', 'ROI mask file path']\n",
            "  patient_id  breast_density left or right breast image view  abnormality id  \\\n",
            "0    P_00001               3                 LEFT         CC               1   \n",
            "1    P_00001               3                 LEFT        MLO               1   \n",
            "2    P_00004               3                 LEFT         CC               1   \n",
            "\n",
            "  abnormality type                          mass shape mass margins  \\\n",
            "0             mass  IRREGULAR-ARCHITECTURAL_DISTORTION   SPICULATED   \n",
            "1             mass  IRREGULAR-ARCHITECTURAL_DISTORTION   SPICULATED   \n",
            "2             mass            ARCHITECTURAL_DISTORTION  ILL_DEFINED   \n",
            "\n",
            "   assessment  pathology  subtlety  \\\n",
            "0           4  MALIGNANT         4   \n",
            "1           4  MALIGNANT         4   \n",
            "2           4     BENIGN         3   \n",
            "\n",
            "                                     image file path  \\\n",
            "0  Mass-Training_P_00001_LEFT_CC/1.3.6.1.4.1.9590...   \n",
            "1  Mass-Training_P_00001_LEFT_MLO/1.3.6.1.4.1.959...   \n",
            "2  Mass-Training_P_00004_LEFT_CC/1.3.6.1.4.1.9590...   \n",
            "\n",
            "                             cropped image file path  \\\n",
            "0  Mass-Training_P_00001_LEFT_CC_1/1.3.6.1.4.1.95...   \n",
            "1  Mass-Training_P_00001_LEFT_MLO_1/1.3.6.1.4.1.9...   \n",
            "2  Mass-Training_P_00004_LEFT_CC_1/1.3.6.1.4.1.95...   \n",
            "\n",
            "                                  ROI mask file path  \n",
            "0  Mass-Training_P_00001_LEFT_CC_1/1.3.6.1.4.1.95...  \n",
            "1  Mass-Training_P_00001_LEFT_MLO_1/1.3.6.1.4.1.9...  \n",
            "2  Mass-Training_P_00004_LEFT_CC_1/1.3.6.1.4.1.95...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_EXtnTyxKsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build mapping from UID folder -> label\n",
        "uid_to_label = {}\n",
        "\n",
        "def add_uid_mapping(df, label_col='pathology', path_col='image file path'):\n",
        "    for _, row in df.iterrows():\n",
        "        path_str = str(row[path_col]).strip()\n",
        "        if not path_str or path_str.lower() == 'nan':\n",
        "            continue\n",
        "        parts = path_str.split('/')\n",
        "        if len(parts) < 3:\n",
        "            continue\n",
        "        # second-to-last folder in CSV path is the JPEG folder name\n",
        "        uid_folder = parts[-2]\n",
        "        label = str(row[label_col]).strip().lower()\n",
        "        uid_to_label[uid_folder] = 1 if label == 'malignant' else 0\n",
        "\n",
        "add_uid_mapping(mass_df)\n",
        "add_uid_mapping(calc_df)\n",
        "\n",
        "print(f\"Total UID mappings: {len(uid_to_label)}\")\n",
        "\n",
        "# Match JPEG files\n",
        "image_records = []\n",
        "missed = 0\n",
        "\n",
        "for patient_uid in os.listdir(BASE_DIR):\n",
        "    patient_dir = os.path.join(BASE_DIR, patient_uid)\n",
        "    if not os.path.isdir(patient_dir):\n",
        "        continue\n",
        "    if patient_uid in uid_to_label:\n",
        "        label_int = uid_to_label[patient_uid]\n",
        "        for fn in os.listdir(patient_dir):\n",
        "            if fn.lower().endswith('.jpg'):\n",
        "                image_records.append((os.path.join(patient_dir, fn), label_int))\n",
        "    else:\n",
        "        missed += 1\n",
        "\n",
        "print(f\"Matched JPEG images: {len(image_records)}\")\n",
        "print(f\"Unmatched UID folders: {missed}\")\n",
        "print(\"Sample matches:\", image_records[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wP9PK4g9xKuY",
        "outputId": "efab410c-788a-46d7-9cd7-c3bc2b85196f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total UID mappings: 2458\n",
            "Matched JPEG images: 2458\n",
            "Unmatched UID folders: 4316\n",
            "Sample matches: [('/content/drive/MyDrive/archive/jpeg/1.3.6.1.4.1.9590.100.1.2.71928301212219609314072252602966948877/1-081.jpg', 0), ('/content/drive/MyDrive/archive/jpeg/1.3.6.1.4.1.9590.100.1.2.182177787312682499236296341930939113234/1-060.jpg', 0), ('/content/drive/MyDrive/archive/jpeg/1.3.6.1.4.1.9590.100.1.2.412935186912567939930947831442475559499/1-093.jpg', 1), ('/content/drive/MyDrive/archive/jpeg/1.3.6.1.4.1.9590.100.1.2.75074886911317428336210351841116845645/1-123.jpg', 1), ('/content/drive/MyDrive/archive/jpeg/1.3.6.1.4.1.9590.100.1.2.398436206112075190335811098902545958874/1-243.jpg', 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "odiUkeL7xKwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# === Paths ===\n",
        "jpeg_root = \"/content/drive/MyDrive/archive/jpeg\"\n",
        "mass_csv_path = \"/content/drive/MyDrive/archive/csv/mass_case_description_train_set.csv\"\n",
        "calc_csv_path = \"/content/drive/MyDrive/archive/csv/calc_case_description_train_set.csv\"\n",
        "\n",
        "# === Load CSVs ===\n",
        "mass_df = pd.read_csv(mass_csv_path)\n",
        "calc_df = pd.read_csv(calc_csv_path)\n",
        "\n",
        "# Combine CSVs\n",
        "all_csv_paths = pd.concat([mass_df['image file path'], calc_df['image file path']]).tolist()\n",
        "\n",
        "# === Build UID mappings from CSVs (CORRECT LEVEL) ===\n",
        "uid_mappings = {}\n",
        "for path in all_csv_paths:\n",
        "    parts = path.split('/')\n",
        "    if len(parts) >= 2:\n",
        "        uid = parts[-2]  # folder before the DICOM file\n",
        "        label = 0 if 'calc' in path.lower() else 1\n",
        "        uid_mappings[uid] = label\n",
        "\n",
        "print(\"Total UID mappings:\", len(uid_mappings))\n",
        "\n",
        "# === Compare with JPEG folder ===\n",
        "jpeg_uids = set(os.listdir(jpeg_root))\n",
        "matched_uids = set(uid_mappings.keys()) & jpeg_uids\n",
        "unmatched_uids = jpeg_uids - matched_uids\n",
        "\n",
        "print(\"Matched JPEG images:\", len(matched_uids))\n",
        "print(\"Unmatched UID folders:\", len(unmatched_uids))\n",
        "print(\"Sample matched:\", list(matched_uids)[:5])\n",
        "print(\"Sample unmatched:\", list(unmatched_uids)[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClOgJxyzxKyd",
        "outputId": "9327dd64-c55d-43f0-f655-f639249308ef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total UID mappings: 2458\n",
            "Matched JPEG images: 2458\n",
            "Unmatched UID folders: 4317\n",
            "Sample matched: ['1.3.6.1.4.1.9590.100.1.2.106917101213622946607766232832878006143', '1.3.6.1.4.1.9590.100.1.2.247139233113856247217565715372804656493', '1.3.6.1.4.1.9590.100.1.2.214159660811754034822231515062693367987', '1.3.6.1.4.1.9590.100.1.2.93262706812708085503412661002234752608', '1.3.6.1.4.1.9590.100.1.2.117311526211769399912345133122658800626']\n",
            "Sample unmatched: ['1.3.6.1.4.1.9590.100.1.2.304902787311311619535166611670640395210', '1.3.6.1.4.1.9590.100.1.2.318836487510466400123723183690561506321', '1.3.6.1.4.1.9590.100.1.2.323441279313490933607793228002205134456', '1.3.6.1.4.1.9590.100.1.2.136770106912670810407098991750285575490', '1.3.6.1.4.1.9590.100.1.2.142970723512367328629685306071766834363']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# === Paths ===\n",
        "jpeg_root = \"/content/drive/MyDrive/archive/jpeg\"\n",
        "csv_dir = \"/content/drive/MyDrive/archive/csv\"\n",
        "\n",
        "# === Load ALL CSVs ===\n",
        "mass_train = pd.read_csv(os.path.join(csv_dir, \"mass_case_description_train_set.csv\"))\n",
        "mass_test  = pd.read_csv(os.path.join(csv_dir, \"mass_case_description_test_set.csv\"))\n",
        "calc_train = pd.read_csv(os.path.join(csv_dir, \"calc_case_description_train_set.csv\"))\n",
        "calc_test  = pd.read_csv(os.path.join(csv_dir, \"calc_case_description_test_set.csv\"))\n",
        "\n",
        "# Combine them all\n",
        "all_df = pd.concat([mass_train, mass_test, calc_train, calc_test], ignore_index=True)\n",
        "\n",
        "# === Build UID mappings from CSVs ===\n",
        "uid_mappings = {}\n",
        "for path in all_df['image file path']:\n",
        "    parts = path.split('/')\n",
        "    if len(parts) >= 2:\n",
        "        uid = parts[-2]  # final UID folder before the DICOM filename\n",
        "        label_str = path.lower()\n",
        "        # label 1 for malignant, 0 for benign\n",
        "        if 'malignant' in label_str:\n",
        "            label = 1\n",
        "        else:\n",
        "            label = 0\n",
        "        uid_mappings[uid] = label\n",
        "\n",
        "print(\"Total UID mappings:\", len(uid_mappings))\n",
        "\n",
        "# === Compare with JPEG folder ===\n",
        "jpeg_uids = set(os.listdir(jpeg_root))\n",
        "matched_uids = set(uid_mappings.keys()) & jpeg_uids\n",
        "unmatched_uids = jpeg_uids - matched_uids\n",
        "\n",
        "print(\"Matched JPEG images:\", len(matched_uids))\n",
        "print(\"Unmatched UID folders:\", len(unmatched_uids))\n",
        "print(\"Sample matched:\", list(matched_uids)[:5])\n",
        "print(\"Sample unmatched:\", list(unmatched_uids)[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSh5fX74xK0i",
        "outputId": "931e2b56-bb5b-4e77-89b1-bc4a466d8cfd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total UID mappings: 3103\n",
            "Matched JPEG images: 3103\n",
            "Unmatched UID folders: 3672\n",
            "Sample matched: ['1.3.6.1.4.1.9590.100.1.2.318836487510466400123723183690561506321', '1.3.6.1.4.1.9590.100.1.2.106917101213622946607766232832878006143', '1.3.6.1.4.1.9590.100.1.2.247139233113856247217565715372804656493', '1.3.6.1.4.1.9590.100.1.2.272917492411709393015036949944104292812', '1.3.6.1.4.1.9590.100.1.2.214159660811754034822231515062693367987']\n",
            "Sample unmatched: ['1.3.6.1.4.1.9590.100.1.2.304902787311311619535166611670640395210', '1.3.6.1.4.1.9590.100.1.2.323441279313490933607793228002205134456', '1.3.6.1.4.1.9590.100.1.2.136770106912670810407098991750285575490', '1.3.6.1.4.1.9590.100.1.2.142970723512367328629685306071766834363', '1.3.6.1.4.1.9590.100.1.2.195924545611755277629845871882119525289']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Directory for filtered dataset\n",
        "filtered_dir = \"/content/drive/MyDrive/archive/filtered_jpeg\"\n",
        "os.makedirs(filtered_dir, exist_ok=True)\n",
        "\n",
        "for uid in matched_uids:\n",
        "    src_dir = os.path.join(jpeg_root, uid)\n",
        "    dst_dir = os.path.join(filtered_dir, uid)\n",
        "    if not os.path.exists(dst_dir):\n",
        "        shutil.copytree(src_dir, dst_dir)\n",
        "\n",
        "print(\"Filtered dataset ready at:\", filtered_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfykDq8RxK2U",
        "outputId": "7ab42102-5b04-421a-ba71-8335c0e1fef6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered dataset ready at: /content/drive/MyDrive/archive/filtered_jpeg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "jpeg_root = \"/content/drive/MyDrive/archive/jpeg\"\n",
        "matched_uids = set(uid_mappings.keys())  # from your matching script\n",
        "\n",
        "removed_count = 0\n",
        "\n",
        "for folder in os.listdir(jpeg_root):\n",
        "    folder_path = os.path.join(jpeg_root, folder)\n",
        "    if os.path.isdir(folder_path) and folder not in matched_uids:\n",
        "        shutil.rmtree(folder_path)\n",
        "        removed_count += 1\n",
        "\n",
        "print(f\"Removed {removed_count} unmatched UID folders.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y6ZTrifxK4i",
        "outputId": "f80a4d95-920c-4de9-a7fc-2d9e857edf74"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed 3671 unmatched UID folders.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHT4FrP0xK6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create stratified train/val/test split\n",
        "paths = [r[0] for r in image_records]\n",
        "labels = [r[1] for r in image_records]\n",
        "\n",
        "if len(paths) < 10:\n",
        "    raise RuntimeError(\"Too few images matched. Check CSV mapping and BASE_DIR paths.\")\n",
        "\n",
        "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "    paths, labels, test_size=0.2, random_state=42, stratify=labels)\n",
        "\n",
        "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "    temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels)\n",
        "\n",
        "train_records = list(zip(train_paths, train_labels))\n",
        "val_records = list(zip(val_paths, val_labels))\n",
        "test_records = list(zip(test_paths, test_labels))\n",
        "\n",
        "print(\"Splits -> train:\", len(train_records), \"val:\", len(val_records), \"test:\", len(test_records))\n",
        "print(\"Train positive ratio:\", np.mean([l for _,l in train_records]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu0ilycFxK9K",
        "outputId": "634b0873-bad7-46eb-c544-39e875431df3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splits -> train: 1966 val: 246 test: 246\n",
            "Train positive ratio: 0.4496439471007121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cell 6: Create stratified train/val/test split ---\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Ensure we have data\n",
        "if not image_records or len(image_records) < 10:\n",
        "    raise RuntimeError(f\"Too few images matched ({len(image_records)}). \"\n",
        "                       \"Check CSV mapping and filtered dataset path.\")\n",
        "\n",
        "# Separate paths and labels\n",
        "paths  = np.array([rec[0] for rec in image_records])\n",
        "labels = np.array([rec[1] for rec in image_records])\n",
        "\n",
        "# Stratified Train/Test split (80% train, 20% temp)\n",
        "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "    paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Stratified Temp -> Val/Test split (50% val, 50% test from temp)\n",
        "val_paths, test_paths, val_labels, test_labels = train_test_split(\n",
        "    temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "# Combine into records\n",
        "train_records = list(zip(train_paths, train_labels))\n",
        "val_records   = list(zip(val_paths, val_labels))\n",
        "test_records  = list(zip(test_paths, test_labels))\n",
        "\n",
        "# Print summary\n",
        "print(f\"Splits -> train: {len(train_records)}, val: {len(val_records)}, test: {len(test_records)}\")\n",
        "print(f\"Train positive ratio: {np.mean([lbl for _, lbl in train_records]):.3f}\")\n",
        "print(f\"Val   positive ratio: {np.mean([lbl for _, lbl in val_records]):.3f}\")\n",
        "print(f\"Test  positive ratio: {np.mean([lbl for _, lbl in test_records]):.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wAkgvur21jP",
        "outputId": "9978c786-80fd-4180-c8b3-32a681d57c39"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splits -> train: 1966, val: 246, test: 246\n",
            "Train positive ratio: 0.450\n",
            "Val   positive ratio: 0.451\n",
            "Test  positive ratio: 0.447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Cell 7 ----------------\n",
        "# Dataset class, transforms & DataLoaders (improved)\n",
        "import os, random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "\n",
        "# Output folder for figures\n",
        "OUT_DIR = \"/content/drive/MyDrive/cbis_ddsm_results\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "class MammogramDataset(Dataset):\n",
        "    def __init__(self, records, transform=None):\n",
        "        \"\"\"\n",
        "        records: list of (path, label_int)\n",
        "        transform: torchvision transforms to apply\n",
        "        \"\"\"\n",
        "        self.records = list(records)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.records[idx]\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            # fallback: black image\n",
        "            print(f\"Warning: couldn't open {path!r}: {e}\")\n",
        "            img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), (0,0,0))\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# Image size (keep 224 for ViT-base patch16_224)\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# stronger augmentation for training\n",
        "train_transform = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.RandomHorizontalFlip(0.5),\n",
        "    T.RandomVerticalFlip(0.1),\n",
        "    T.RandomRotation(10),\n",
        "    T.ColorJitter(brightness=0.12, contrast=0.12, saturation=0.06),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# minimal transforms for val/test\n",
        "eval_transform = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# Create dataset objects (expects train_records / val_records / test_records defined)\n",
        "train_ds = MammogramDataset(train_records, transform=train_transform)\n",
        "val_ds   = MammogramDataset(val_records, transform=eval_transform)\n",
        "test_ds  = MammogramDataset(test_records, transform=eval_transform)\n",
        "\n",
        "BATCH_SIZE = 16   # reduce if OOM\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print(\"Dataset sizes: train\", len(train_ds), \"val\", len(val_ds), \"test\", len(test_ds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djEWAnI64td-",
        "outputId": "d8ad797f-9464-4804-a049-dff5b56c0f5d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset sizes: train 1966 val 246 test 246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Cell 8 ----------------\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm import create_model\n",
        "\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, vit_name='vit_base_patch16_224', regnet_name='regnety_032', pretrained=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.vit = create_model(vit_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
        "        self.regnet = create_model(regnet_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
        "\n",
        "        # try to detect num_features\n",
        "        self.vit_feat = getattr(self.vit, 'num_features', None) or getattr(self.vit, 'embed_dim', None) or 768\n",
        "        self.regnet_feat = getattr(self.regnet, 'num_features', None) or getattr(self.regnet, 'head', None) and getattr(self.regnet.head, 'in_features', None) or 1024\n",
        "\n",
        "        fused_dim = int(self.vit_feat + self.regnet_feat)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fused_dim, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 1)  # binary logit\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        v = self.vit.forward_features(x)\n",
        "        r = self.regnet.forward_features(x)\n",
        "\n",
        "        # ensure global pooled vectors\n",
        "        if v.dim() > 2:\n",
        "            if v.dim() == 3:  # [B, N, C] -> avg over tokens\n",
        "                v = v.mean(dim=1)\n",
        "            else:\n",
        "                v = F.adaptive_avg_pool2d(v, 1).reshape(v.size(0), -1)\n",
        "        if r.dim() > 2:\n",
        "            if r.dim() == 3:\n",
        "                r = r.mean(dim=1)\n",
        "            else:\n",
        "                r = F.adaptive_avg_pool2d(r, 1).reshape(r.size(0), -1)\n",
        "\n",
        "        x = torch.cat([v, r], dim=1)\n",
        "        logits = self.classifier(x).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "# build and move to device\n",
        "model = FusionModel(vit_name='vit_base_patch16_224', regnet_name='regnety_032', pretrained=True, dropout=0.3)\n",
        "model = model.to(device)\n",
        "print(\"Model built. vit_feat, regnet_feat:\", model.vit_feat, model.regnet_feat)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244,
          "referenced_widgets": [
            "84379a7be43a40bb820a50e943f73bdd",
            "cc2dd57ebdd546179c69fd95f046c0ac",
            "163f112ed04c484392f4dbe604d866d5",
            "4aef04cfb2464e42a10dff5f52fd05d4",
            "33141610f947480e89379a9f1f436bb1",
            "c04856d02fc3404aab8f30901c1d250b",
            "d068930bfe064d8f8439f065eae62b45",
            "f4a3c9733a38427d8cc9fde41c6b6985",
            "956e2108d0764abbacbbcd2a60043cac",
            "dee800d9cd7d4169b4bce3c035f91cf6",
            "e54e21ea9db44714820845bac37d1950",
            "f2b1d630cb1445f6a157f3d36b9e94fe",
            "6cc4d9af15da4dec96eafb3a6cacb28c",
            "2b5d407d52b441a6941ecd2eeb464090",
            "3a93977b62b84a9ba36d862aeaeb29e7",
            "ecbec6ea2cec4fb295fddddea0ffbff6",
            "872380b97de34b439bb4467e71cbcadb",
            "8fcc52106df142fdb7d5e173af15794a",
            "d5af80c161b946ec9aca62438d37a1f2",
            "2bc3735d20714593bf7de579263f9044",
            "99b76c05f83a42f5991f81e2cc2e924a",
            "534d01f89e1d4ac69616e60f97e65af9"
          ]
        },
        "id": "cVz8dnUw4tfm",
        "outputId": "2f28a290-007d-4106-ef18-d5668dd721bd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84379a7be43a40bb820a50e943f73bdd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (norm.bias, norm.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/78.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2b1d630cb1445f6a157f3d36b9e94fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built. vit_feat, regnet_feat: 768 1512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Cell 9 ----------------\n",
        "from torch.optim import AdamW\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, auc\n",
        "\n",
        "# compute pos_weight (safe)\n",
        "train_pos = sum([lbl for _,lbl in train_records])\n",
        "train_neg = len(train_records) - train_pos\n",
        "if train_pos == 0:\n",
        "    raise RuntimeError(\"No positive samples in training set!\")\n",
        "pos_weight = torch.tensor([(train_neg / train_pos) if train_pos > 0 else 1.0], dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "# training bookkeeping\n",
        "history = {\n",
        "    'train_loss': [], 'train_auc': [], 'train_acc': [],\n",
        "    'val_loss': [], 'val_auc': [], 'val_acc': [],\n",
        "    'val_precision': [], 'val_recall': [], 'val_f1': []\n",
        "}\n",
        "\n",
        "def sigmoid_np(x):\n",
        "    return 1.0/(1.0+np.exp(-x))\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for imgs, labels in loader:\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "        all_preds.extend(probs.tolist())\n",
        "        all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    try:\n",
        "        epoch_auc = roc_auc_score(all_labels, all_preds)\n",
        "    except Exception:\n",
        "        epoch_auc = float('nan')\n",
        "    preds_bin = [1 if p>0.5 else 0 for p in all_preds]\n",
        "    epoch_acc = accuracy_score(all_labels, preds_bin)\n",
        "    return epoch_loss, epoch_auc, epoch_acc\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in loader:\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            all_preds.extend(probs.tolist())\n",
        "            all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
        "    val_loss = running_loss / len(loader.dataset)\n",
        "    try:\n",
        "        val_auc = roc_auc_score(all_labels, all_preds)\n",
        "    except Exception:\n",
        "        val_auc = float('nan')\n",
        "    preds_bin = [1 if p>0.5 else 0 for p in all_preds]\n",
        "    try:\n",
        "        val_acc = accuracy_score(all_labels, preds_bin)\n",
        "        val_precision = precision_score(all_labels, preds_bin, zero_division=0)\n",
        "        val_recall = recall_score(all_labels, preds_bin, zero_division=0)\n",
        "        val_f1 = f1_score(all_labels, preds_bin, zero_division=0)\n",
        "    except Exception:\n",
        "        val_acc = val_precision = val_recall = val_f1 = float('nan')\n",
        "    return val_loss, val_auc, val_acc, val_precision, val_recall, val_f1, all_labels, all_preds\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4KNdMlq4thV",
        "outputId": "cbfef5df-8618-4e55-a867-fc6e8c195535"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-2368088187.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Cell 10 ----------------\n",
        "import time, math\n",
        "EPOCHS = 10\n",
        "best_val_auc = -1.0\n",
        "save_path = os.path.join(OUT_DIR, 'cbis_ddsm_fusion_best.pth')\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    train_loss, train_auc, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
        "    val_loss, val_auc, val_acc, val_prec, val_rec, val_f1, _, _ = validate(model, val_loader, criterion, device)\n",
        "    scheduler.step(val_auc if not math.isnan(val_auc) else val_loss)\n",
        "\n",
        "    # record to history\n",
        "    history['train_loss'].append(train_loss); history['train_auc'].append(train_auc); history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss); history['val_auc'].append(val_auc); history['val_acc'].append(val_acc)\n",
        "    history['val_precision'].append(val_prec); history['val_recall'].append(val_rec); history['val_f1'].append(val_f1)\n",
        "\n",
        "    t1 = time.time()\n",
        "    print(f\"Epoch {epoch}/{EPOCHS} time={t1-t0:.1f}s train_loss={train_loss:.4f} train_auc={train_auc:.4f} train_acc={train_acc:.4f} val_loss={val_loss:.4f} val_auc={val_auc:.4f} val_acc={val_acc:.4f} val_f1={val_f1:.4f}\")\n",
        "\n",
        "    # Save best\n",
        "    if not math.isnan(val_auc) and val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'val_auc': val_auc, 'history': history}, save_path)\n",
        "        print(\"Saved best model to:\", save_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwhgYQtd4tjP",
        "outputId": "9ea6db41-5b6c-497a-d59e-cef697ca61d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 time=863.0s train_loss=0.7831 train_auc=0.5959 train_acc=0.5605 val_loss=0.8178 val_auc=0.6289 val_acc=0.5203 val_f1=0.6467\n",
            "Saved best model to: /content/drive/MyDrive/cbis_ddsm_results/cbis_ddsm_fusion_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 time=381.8s train_loss=0.6966 train_auc=0.6945 train_acc=0.6236 val_loss=0.6940 val_auc=0.7311 val_acc=0.6341 val_f1=0.6831\n",
            "Saved best model to: /content/drive/MyDrive/cbis_ddsm_results/cbis_ddsm_fusion_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 time=408.2s train_loss=0.6422 train_auc=0.7569 train_acc=0.6780 val_loss=0.6312 val_auc=0.7734 val_acc=0.6951 val_f1=0.6964\n",
            "Saved best model to: /content/drive/MyDrive/cbis_ddsm_results/cbis_ddsm_fusion_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 time=411.4s train_loss=0.6073 train_auc=0.7830 train_acc=0.6882 val_loss=0.6412 val_auc=0.7888 val_acc=0.6870 val_f1=0.6351\n",
            "Saved best model to: /content/drive/MyDrive/cbis_ddsm_results/cbis_ddsm_fusion_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 time=406.8s train_loss=0.5658 train_auc=0.8228 train_acc=0.7309 val_loss=0.8374 val_auc=0.7622 val_acc=0.6870 val_f1=0.5882\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 time=375.6s train_loss=0.5526 train_auc=0.8339 train_acc=0.7467 val_loss=0.6305 val_auc=0.7841 val_acc=0.6789 val_f1=0.6973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 time=376.7s train_loss=0.5198 train_auc=0.8554 train_acc=0.7675 val_loss=0.6485 val_auc=0.7767 val_acc=0.7033 val_f1=0.7068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 time=374.2s train_loss=0.4414 train_auc=0.8989 train_acc=0.8133 val_loss=0.6953 val_auc=0.7915 val_acc=0.6911 val_f1=0.6162\n",
            "Saved best model to: /content/drive/MyDrive/cbis_ddsm_results/cbis_ddsm_fusion_best.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 time=404.4s train_loss=0.3966 train_auc=0.9188 train_acc=0.8276 val_loss=0.7795 val_auc=0.7893 val_acc=0.7073 val_f1=0.6842\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2368088187.py:36: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 time=379.2s train_loss=0.3655 train_auc=0.9315 train_acc=0.8505 val_loss=0.7548 val_auc=0.7760 val_acc=0.6748 val_f1=0.6154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Cell 11 ----------------\n",
        "import torch\n",
        "from sklearn.metrics import precision_recall_curve, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ckpt = torch.load(save_path, map_location=device, weights_only=False)\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "print(\"Loaded checkpoint epoch\", ckpt.get('epoch'), \"val_auc\", ckpt.get('val_auc'))\n",
        "\n",
        "# Evaluate on test set and collect preds\n",
        "model.eval()\n",
        "test_loss = 0.0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        test_loss += loss.item() * imgs.size(0)\n",
        "        probs = torch.sigmoid(logits).cpu().numpy().tolist()\n",
        "        all_preds.extend(probs)\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "\n",
        "avg_test_loss = test_loss / len(test_loader.dataset)\n",
        "test_auc = roc_auc_score(all_labels, all_preds)\n",
        "preds_bin = [1 if p>0.5 else 0 for p in all_preds]\n",
        "test_acc = accuracy_score(all_labels, preds_bin)\n",
        "test_prec = precision_score(all_labels, preds_bin, zero_division=0)\n",
        "test_rec = recall_score(all_labels, preds_bin, zero_division=0)\n",
        "test_f1 = f1_score(all_labels, preds_bin, zero_division=0)\n",
        "\n",
        "print(f\"Test loss: {avg_test_loss:.4f}  AUC: {test_auc:.4f}  Acc: {test_acc:.4f}  Prec: {test_prec:.4f}  Rec: {test_rec:.4f}  F1: {test_f1:.4f}\")\n",
        "\n",
        "# --- Precision-Recall curve (HD) ---\n",
        "precision, recall, thresholds = precision_recall_curve(all_labels, all_preds)\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "plt.figure(figsize=(8,6), dpi=300)\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.plot(recall, precision, color='#1f77b4', linewidth=2)\n",
        "plt.fill_between(recall, precision, alpha=0.15, color='#1f77b4')\n",
        "plt.xlabel(\"Recall\", fontsize=14)\n",
        "plt.ylabel(\"Precision\", fontsize=14)\n",
        "plt.title(f\"Precision-Recall curve (AUC={pr_auc:.4f})\", fontsize=16)\n",
        "plt.xlim(0,1); plt.ylim(0,1)\n",
        "plt.tight_layout()\n",
        "pr_path = os.path.join(OUT_DIR, \"precision_recall_curve.png\")\n",
        "plt.savefig(pr_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved PR curve:\", pr_path)\n",
        "\n",
        "# --- Confusion Matrix (HD) ---\n",
        "cm = confusion_matrix(all_labels, preds_bin)\n",
        "plt.figure(figsize=(6,5), dpi=300)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={\"size\":14})\n",
        "plt.xlabel(\"Predicted\", fontsize=12); plt.ylabel(\"Actual\", fontsize=12)\n",
        "plt.title(f\"Confusion Matrix (Acc={test_acc:.3f}, F1={test_f1:.3f})\", fontsize=14)\n",
        "cm_path = os.path.join(OUT_DIR, \"confusion_matrix.png\")\n",
        "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved Confusion Matrix:\", cm_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYaIGz4J4tlU",
        "outputId": "4d2061da-d82c-4f8a-8bd7-8c3a6f878f98"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint epoch 8 val_auc 0.791524858191525\n",
            "Test loss: 0.6548  AUC: 0.8074  Acc: 0.6951  Prec: 0.7011  Rec: 0.5545  F1: 0.6193\n",
            "Saved PR curve: /content/drive/MyDrive/cbis_ddsm_results/precision_recall_curve.png\n",
            "Saved Confusion Matrix: /content/drive/MyDrive/cbis_ddsm_results/confusion_matrix.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Cell 12 ----------------\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "epochs = np.arange(1, len(history['train_loss'])+1)\n",
        "\n",
        "# Loss curve\n",
        "plt.figure(figsize=(10,6), dpi=300)\n",
        "plt.plot(epochs, history['train_loss'], label='Train Loss', color='#d62728', linewidth=2)\n",
        "plt.plot(epochs, history['val_loss'], label='Val Loss', color='#2ca02c', linewidth=2)\n",
        "plt.scatter(epochs, history['train_loss'], color='#d62728', s=20)\n",
        "plt.scatter(epochs, history['val_loss'], color='#2ca02c', s=20)\n",
        "plt.xlabel(\"Epoch\", fontsize=14); plt.ylabel(\"Loss\", fontsize=14)\n",
        "plt.title(\"Train & Validation Loss\", fontsize=16)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "loss_path = os.path.join(OUT_DIR, \"loss_curve.png\")\n",
        "plt.savefig(loss_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved Loss curve:\", loss_path)\n",
        "\n",
        "# Accuracy curve\n",
        "plt.figure(figsize=(10,6), dpi=300)\n",
        "plt.plot(epochs, history['train_acc'], label='Train Accuracy', color='#9467bd', linewidth=2)\n",
        "plt.plot(epochs, history['val_acc'], label='Val Accuracy', color='#8c564b', linewidth=2)\n",
        "plt.xlabel(\"Epoch\", fontsize=14); plt.ylabel(\"Accuracy\", fontsize=14)\n",
        "plt.title(\"Train & Validation Accuracy\", fontsize=16)\n",
        "plt.legend(fontsize=12)\n",
        "plt.tight_layout()\n",
        "acc_path = os.path.join(OUT_DIR, \"accuracy_curve.png\")\n",
        "plt.savefig(acc_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved Accuracy curve:\", acc_path)\n",
        "\n",
        "# F1 curve (val)\n",
        "plt.figure(figsize=(10,6), dpi=300)\n",
        "plt.plot(epochs, history['val_f1'], label='Val F1', color='#17becf', linewidth=2)\n",
        "plt.scatter(epochs, history['val_f1'], color='#17becf', s=20)\n",
        "plt.xlabel(\"Epoch\", fontsize=14); plt.ylabel(\"F1 Score\", fontsize=14)\n",
        "plt.title(\"Validation F1 Score\", fontsize=16)\n",
        "plt.tight_layout()\n",
        "f1_path = os.path.join(OUT_DIR, \"f1_curve.png\")\n",
        "plt.savefig(f1_path, dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"Saved F1 curve:\", f1_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o10_frY4tnX",
        "outputId": "54b9bd45-e0b6-4fe6-e779-f99ececc15c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Loss curve: /content/drive/MyDrive/cbis_ddsm_results/loss_curve.png\n",
            "Saved Accuracy curve: /content/drive/MyDrive/cbis_ddsm_results/accuracy_curve.png\n",
            "Saved F1 curve: /content/drive/MyDrive/cbis_ddsm_results/f1_curve.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URQPCGS9NeDw",
        "outputId": "70b3163f-a33c-462d-92a4-e58bb7f9388a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------- Cell 13 (REPLACEMENT) ----------------\n",
        "# Manual Grad-CAM hooks -> robust across pytorch-grad-cam versions\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "# Output file\n",
        "OUT_DIR = \"/content/drive/MyDrive/cbis_ddsm_results\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "# Define image size before Grad-CAM cell\n",
        "IMG_SIZE = 224\n",
        "# Helpers\n",
        "def preprocess_pil_for_model(pil_img, size=IMG_SIZE):\n",
        "    pil_resized = pil_img.resize((size, size))\n",
        "    # return both normalized tensor (for model) and original RGB array (for overlay)\n",
        "    t = eval_transform(pil_resized)  # normalized tensor (C,H,W)\n",
        "    rgb = np.asarray(pil_resized).astype(np.float32) / 255.0  # HWC 0..1\n",
        "    return t.unsqueeze(0).to(device), rgb\n",
        "\n",
        "def compute_gradcam_single_layer(model, input_tensor, target_layer, target_label=1):\n",
        "    \"\"\"\n",
        "    Compute Grad-CAM heatmap (H x W numpy, values 0..1) for a single Conv2d target_layer.\n",
        "    target_label: 1 -> positive logit, 0 -> complementary (we take -logit)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    activations = []\n",
        "    gradients = []\n",
        "\n",
        "    # forward hook\n",
        "    def forward_hook(module, inp, out):\n",
        "        activations.append(out)\n",
        "    # backward hook\n",
        "    def backward_hook(module, grad_in, grad_out):\n",
        "        # grad_out is a tuple; grad_out[0] is gradient wrt module output\n",
        "        gradients.append(grad_out[0])\n",
        "\n",
        "    h_fwd = target_layer.register_forward_hook(forward_hook)\n",
        "    # register full backward hook if available, otherwise fallback\n",
        "    if hasattr(target_layer, \"register_full_backward_hook\"):\n",
        "        h_bwd = target_layer.register_full_backward_hook(lambda m, gi, go: gradients.append(go[0]))\n",
        "    else:\n",
        "        h_bwd = target_layer.register_backward_hook(lambda m, gi, go: gradients.append(go[0]))\n",
        "\n",
        "    # Forward\n",
        "    model.zero_grad()\n",
        "    out = model(input_tensor)  # expected shape [1] (single logit)\n",
        "    if out is None:\n",
        "        h_fwd.remove(); h_bwd.remove()\n",
        "        raise RuntimeError(\"Model returned None. Check forward pass.\")\n",
        "    # out shape might be [1] or [1,] or [1,1]; get scalar logit\n",
        "    logit = out.reshape(-1)[0]\n",
        "\n",
        "    # For binary single-logit model:\n",
        "    # - to get gradients for class 1 (malignant) differentiate logit\n",
        "    # - to get gradients for class 0 differentiate -logit\n",
        "    if int(target_label) == 1:\n",
        "        score = logit\n",
        "    else:\n",
        "        score = -logit\n",
        "\n",
        "    score.backward(retain_graph=True)\n",
        "\n",
        "    # Ensure we captured something\n",
        "    if len(activations) == 0 or len(gradients) == 0:\n",
        "        h_fwd.remove(); h_bwd.remove()\n",
        "        raise RuntimeError(\"Failed to capture activations or gradients. Hook did not fire.\")\n",
        "\n",
        "    act = activations[-1].detach()   # tensor [1, C, H, W]\n",
        "    grad = gradients[-1].detach()    # tensor [1, C, H, W]\n",
        "\n",
        "    # Global average pooling of gradients -> weights\n",
        "    weights = torch.mean(grad, dim=(2,3), keepdim=True)   # [1,C,1,1]\n",
        "    # Weighted combination\n",
        "    cam = torch.sum(weights * act, dim=1).squeeze(0)  # [H, W]\n",
        "    cam = F.relu(cam)\n",
        "\n",
        "    # Normalize to 0..1\n",
        "    cam -= cam.min()\n",
        "    if cam.max() > 0:\n",
        "        cam = cam / cam.max()\n",
        "    cam_np = cam.cpu().numpy()\n",
        "\n",
        "    # remove hooks\n",
        "    h_fwd.remove(); h_bwd.remove()\n",
        "    return cam_np\n",
        "\n",
        "def upsample_cam(cam, size=(IMG_SIZE, IMG_SIZE)):\n",
        "    # cam is HxW float 0..1 -> resize to size\n",
        "    cam_resized = cv2.resize(cam, (size[1], size[0]), interpolation=cv2.INTER_LINEAR)\n",
        "    cam_resized = np.clip(cam_resized, 0, 1)\n",
        "    return cam_resized\n",
        "\n",
        "def apply_colormap_on_image(img_rgb, cam, colormap=cv2.COLORMAP_JET, alpha=0.5):\n",
        "    \"\"\"\n",
        "    img_rgb: HxWx3 float 0..1\n",
        "    cam: HxW float 0..1\n",
        "    returns overlay_rgb float 0..1\n",
        "    \"\"\"\n",
        "    heatmap_255 = np.uint8(255 * cam)\n",
        "    heatmap_color = cv2.applyColorMap(heatmap_255, colormap)  # BGR uint8\n",
        "    heatmap_color = cv2.cvtColor(heatmap_color, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
        "    overlay = (1.0 - alpha) * img_rgb + alpha * heatmap_color\n",
        "    overlay = np.clip(overlay, 0, 1)\n",
        "    return heatmap_color, overlay\n",
        "\n",
        "# Determine layers (we already printed them earlier)\n",
        "print(\"vit_last_conv:\", vit_last_conv)\n",
        "print(\"regnet_last_conv:\", regnet_last_conv)\n",
        "if vit_last_conv is None and regnet_last_conv is None:\n",
        "    raise RuntimeError(\"No conv layers found for either backbone; cannot compute Grad-CAM.\")\n",
        "\n",
        "# Pick some examples from test set\n",
        "N = 6\n",
        "samples = test_records[:N]\n",
        "\n",
        "# Create figure large + high DPI\n",
        "fig = plt.figure(figsize=(16, 3 * N), dpi=300)\n",
        "\n",
        "for i, (path, true_label) in enumerate(samples):\n",
        "    pil = Image.open(path).convert('RGB')\n",
        "    input_t, rgb_img = preprocess_pil_for_model(pil, size=IMG_SIZE)  # input_t on device, rgb_img HWC 0..1\n",
        "\n",
        "    # predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logit = model(input_t)\n",
        "        prob = torch.sigmoid(logit).cpu().item()\n",
        "        pred_label = 1 if prob > 0.5 else 0\n",
        "\n",
        "    # Compute cams for available layers and fuse them by averaging\n",
        "    cams = []\n",
        "    if vit_last_conv is not None:\n",
        "        try:\n",
        "            cam_v = compute_gradcam_single_layer(model, input_t, vit_last_conv, target_label=pred_label)\n",
        "            cams.append(upsample_cam(cam_v, size=(IMG_SIZE, IMG_SIZE)))\n",
        "        except Exception as e:\n",
        "            print(\"WARNING: vit cam failed for\", path, \"->\", e)\n",
        "\n",
        "    if regnet_last_conv is not None:\n",
        "        try:\n",
        "            cam_r = compute_gradcam_single_layer(model, input_t, regnet_last_conv, target_label=pred_label)\n",
        "            cams.append(upsample_cam(cam_r, size=(IMG_SIZE, IMG_SIZE)))\n",
        "        except Exception as e:\n",
        "            print(\"WARNING: regnet cam failed for\", path, \"->\", e)\n",
        "\n",
        "    if len(cams) == 0:\n",
        "        raise RuntimeError(\"No CAMs computed for image; both layer CAMs failed.\")\n",
        "\n",
        "    fused_cam = np.mean(np.stack(cams, axis=0), axis=0)  # HxW float 0..1\n",
        "\n",
        "    # Create overlay\n",
        "    heatmap_color, overlay = apply_colormap_on_image(rgb_img, fused_cam, alpha=0.5)\n",
        "\n",
        "    # Plot original and overlay\n",
        "    ax1 = fig.add_subplot(N, 2, 2*i+1)\n",
        "    ax1.imshow(rgb_img); ax1.axis('off')\n",
        "    ax1.set_title(f\"Original (true={true_label})\", fontsize=10)\n",
        "\n",
        "    ax2 = fig.add_subplot(N, 2, 2*i+2)\n",
        "    ax2.imshow(overlay); ax2.axis('off')\n",
        "    ax2.set_title(f\"Pred={pred_label} prob={prob:.3f}\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "out_path = os.path.join(OUT_DIR, \"gradcam_fused_examples.png\")\n",
        "fig.savefig(out_path, dpi=300, bbox_inches='tight')\n",
        "plt.close(fig)\n",
        "print(\"Saved Grad-CAM fused examples to:\", out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "FyiFGNhG4tpY",
        "outputId": "04adfd36-a5e4-42e5-953b-805b9cf23582"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'vit_last_conv' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-35230798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;31m# Determine layers (we already printed them earlier)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"vit_last_conv:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvit_last_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"regnet_last_conv:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregnet_last_conv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvit_last_conv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mregnet_last_conv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vit_last_conv' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0__BSWeY4trX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D_iAMOfk4ttQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7E7Rp8zb4tvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cI-XgPIj4txY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g0KVxgOI4tzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7ifGkva44t1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXKQg2cx4t3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lOWfA_uv4t5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Dataset class and transforms (torchvision)\n",
        "class MammogramDataset(Dataset):\n",
        "    def __init__(self, records, transform=None):\n",
        "        self.records = records\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.records)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.records[idx]\n",
        "        try:\n",
        "            img = Image.open(path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            # in case of read error, return a black image (should rarely happen)\n",
        "            print(\"Warning: couldn't open\", path, \"->\", e)\n",
        "            img = Image.new('RGB', (224,224), (0,0,0))\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "# transforms\n",
        "IMG_SIZE = 224\n",
        "train_transform = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.RandomHorizontalFlip(0.5),\n",
        "    T.RandomRotation(10),\n",
        "    T.ColorJitter(brightness=0.06, contrast=0.06),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "eval_transform = T.Compose([\n",
        "    T.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# create datasets\n",
        "train_ds = MammogramDataset(train_records, transform=train_transform)\n",
        "val_ds = MammogramDataset(val_records, transform=eval_transform)\n",
        "test_ds = MammogramDataset(test_records, transform=eval_transform)\n",
        "\n",
        "BATCH_SIZE = 16   # adjust depending on GPU memory\n",
        "num_workers = 2\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=num_workers, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "7YzMMjzAxLE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: FusionModel - ViT + RegNet features concatenation\n",
        "import torch.nn.functional as F\n",
        "from timm import create_model\n",
        "\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, vit_name='vit_base_patch16_224', regnet_name='regnety_032', pretrained=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        # create backbones with no classification head (num_classes=0)\n",
        "        self.vit = create_model(vit_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
        "        self.regnet = create_model(regnet_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
        "        self.vit_feat = getattr(self.vit, 'num_features', None)\n",
        "        self.regnet_feat = getattr(self.regnet, 'num_features', None)\n",
        "        if self.vit_feat is None or self.regnet_feat is None:\n",
        "            # fallback to attribute names sometimes used\n",
        "            self.vit_feat = getattr(self.vit, 'head', None) and getattr(self.vit.head, 'in_features', None) or 768\n",
        "            self.regnet_feat = getattr(self.regnet, 'head', None) and getattr(self.regnet.head, 'in_features', None) or 1024\n",
        "\n",
        "        fused_dim = int(self.vit_feat + self.regnet_feat)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fused_dim, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 1)   # binary logit\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timm models generally have forward_features\n",
        "        v = self.vit.forward_features(x)\n",
        "        r = self.regnet.forward_features(x)\n",
        "\n",
        "        # ensure vectors (global pooling might still be present)\n",
        "        if v.dim() > 2:\n",
        "            # e.g. [B, N, C] or [B, C, H, W]\n",
        "            v = v.view(v.size(0), v.size(1), -1) if v.dim()==3 else v\n",
        "            # final reduce\n",
        "            v = v.mean(dim=1) if v.dim()==3 else F.adaptive_avg_pool2d(v, 1).reshape(v.size(0), -1)\n",
        "\n",
        "        if r.dim() > 2:\n",
        "            r = r.view(r.size(0), r.size(1), -1) if r.dim()==3 else r\n",
        "            r = r.mean(dim=1) if r.dim()==3 else F.adaptive_avg_pool2d(r, 1).reshape(r.size(0), -1)\n",
        "\n",
        "        x = torch.cat([v, r], dim=1)\n",
        "        logits = self.classifier(x).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "# build model and send to device\n",
        "model = FusionModel(vit_name='vit_base_patch16_224', regnet_name='regnety_032', pretrained=True, dropout=0.3)\n",
        "model = model.to(device)\n",
        "print(\"Model built. vit_feat, regnet_feat:\", model.vit_feat, model.regnet_feat)\n"
      ],
      "metadata": {
        "id": "pelpShLvxLF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Loss, optimizer, scheduler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# compute pos_weight for BCEWithLogitsLoss to address imbalance\n",
        "train_pos = sum([l for _,l in train_records])\n",
        "train_neg = len(train_records) - train_pos\n",
        "if train_pos == 0:\n",
        "    raise RuntimeError(\"No positive samples in training set!\")\n",
        "pos_weight = torch.tensor([(train_neg / train_pos) if train_pos > 0 else 1.0], dtype=torch.float32).to(device)\n",
        "print(\"Train pos/neg:\", train_pos, train_neg, \"pos_weight:\", pos_weight.item())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
        "\n",
        "# mixed precision scaler\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "# helper train/val loops\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, scaler):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for imgs, labels in tqdm(loader, desc=\"Train batches\", leave=False):\n",
        "        imgs = imgs.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "        all_preds.extend(probs.tolist())\n",
        "        all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    try:\n",
        "        epoch_auc = roc_auc_score(all_labels, all_preds)\n",
        "    except Exception:\n",
        "        epoch_auc = float('nan')\n",
        "    return epoch_loss, epoch_auc\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(loader, desc=\"Val batches\", leave=False):\n",
        "            imgs = imgs.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
        "            all_preds.extend(probs.tolist())\n",
        "            all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
        "    val_loss = running_loss / len(loader.dataset)\n",
        "    try:\n",
        "        val_auc = roc_auc_score(all_labels, all_preds)\n",
        "    except Exception:\n",
        "        val_auc = float('nan')\n",
        "    # binary accuracy at 0.5\n",
        "    preds_bin = [1 if p>0.5 else 0 for p in all_preds]\n",
        "    try:\n",
        "        val_acc = accuracy_score(all_labels, preds_bin)\n",
        "    except Exception:\n",
        "        val_acc = float('nan')\n",
        "    return val_loss, val_auc, val_acc, all_labels, all_preds\n"
      ],
      "metadata": {
        "id": "tHriRtuvxLGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Train the model (adjust EPOCHS)\n",
        "EPOCHS = 10\n",
        "best_val_auc = -1.0\n",
        "save_path = '/content/drive/MyDrive/cbis_ddsm_fusion_best.pth'\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    train_loss, train_auc = train_one_epoch(model, train_loader, optimizer, criterion, device, scaler)\n",
        "    val_loss, val_auc, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
        "    scheduler.step(val_auc if not math.isnan(val_auc) else val_loss)\n",
        "    t1 = time.time()\n",
        "    print(f\"Epoch {epoch}/{EPOCHS}  time={t1-t0:.1f}s  train_loss={train_loss:.4f} train_auc={train_auc:.4f}  val_loss={val_loss:.4f} val_auc={val_auc:.4f} val_acc={val_acc:.4f}\")\n",
        "    # Save best\n",
        "    if not math.isnan(val_auc) and val_auc > best_val_auc:\n",
        "        best_val_auc = val_auc\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'val_auc': val_auc},\n",
        "                   save_path)\n",
        "        print(\"Saved best model to:\", save_path)\n"
      ],
      "metadata": {
        "id": "vEZHsbIExLHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Load best model checkpoint and evaluate ===\n",
        "\n",
        "import torch\n",
        "\n",
        "# Ensure device is set (match your training setup)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Path where you saved your best checkpoint during training\n",
        "save_path = \"/content/drive/MyDrive/cbis_ddsm_fusion_best.pth\"  # <-- replace with your actual path if different\n",
        "\n",
        "# Load checkpoint (fix for PyTorch 2.6+ UnpicklingError)\n",
        "ckpt = torch.load(save_path, map_location=device, weights_only=False)\n",
        "\n",
        "# Restore model weights\n",
        "model.load_state_dict(ckpt['model_state_dict'])\n",
        "\n",
        "# (Optional) Restore optimizer if needed\n",
        "if 'optimizer_state_dict' in ckpt:\n",
        "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "\n",
        "# Print checkpoint info\n",
        "print(\"Loaded checkpoint:\")\n",
        "print(f\"  Epoch: {ckpt.get('epoch', 'N/A')}\")\n",
        "print(f\"  Val AUC: {ckpt.get('val_auc', 'N/A')}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "model.eval()\n",
        "test_loss = 0\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:  # assumes test_loader is already defined\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item() * images.size(0)\n",
        "\n",
        "        probs = torch.sigmoid(outputs)\n",
        "        all_preds.extend(probs.detach().cpu().numpy().tolist())\n",
        "        all_labels.extend(labels.detach().cpu().numpy().tolist())\n",
        "\n",
        "\n",
        "avg_loss = test_loss / len(test_loader.dataset)\n",
        "\n",
        "# Calculate metrics\n",
        "test_auc = roc_auc_score(all_labels, all_preds)\n",
        "preds_bin = [1 if p > 0.5 else 0 for p in all_preds]\n",
        "test_acc = accuracy_score(all_labels, preds_bin)\n",
        "\n",
        "\n",
        "print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "print(f\"Test AUC: {test_auc:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "leRggEcpxLIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Helper to find last Conv2d in a given submodule (robust for timm models)\n",
        "import torch.nn as nn\n",
        "\n",
        "def find_last_conv(module):\n",
        "    last_conv = None\n",
        "    for m in module.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            last_conv = m\n",
        "    return last_conv\n",
        "\n",
        "# find convs in both backbones inside the fusion model\n",
        "vit_last_conv = find_last_conv(model.vit)\n",
        "regnet_last_conv = find_last_conv(model.regnet)\n",
        "print(\"vit last conv:\", vit_last_conv)\n",
        "print(\"regnet last conv:\", regnet_last_conv)\n",
        "if vit_last_conv is None or regnet_last_conv is None:\n",
        "    print(\"Warning: could not find last conv in one of the backbones. Grad-CAM may fail.\")\n"
      ],
      "metadata": {
        "id": "u2SpkRw0xLJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Run Grad-CAM (fused from both backbones) on example images from test set\n",
        "# We'll compute a cam from the ViT target conv and the RegNet conv and average them.\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "# Import the new target for binary classification\n",
        "from pytorch_grad_cam.utils.model_targets import BinaryClassifierOutputTarget\n",
        "\n",
        "\n",
        "def tensor_to_numpy_img(tensor):\n",
        "    # expects tensor normalized with ImageNet mean/std\n",
        "    t = tensor.detach().cpu().numpy()\n",
        "    t = np.transpose(t, (1,2,0))  # HWC\n",
        "    mean = np.array([0.485,0.456,0.406])\n",
        "    std = np.array([0.229,0.224,0.225])\n",
        "    img = (t * std + mean)\n",
        "    img = np.clip(img, 0, 1)\n",
        "    return img\n",
        "\n",
        "# pick a few examples\n",
        "n_examples = 4\n",
        "sample_records = test_records[:n_examples]\n",
        "\n",
        "# use_cuda = torch.cuda.is_available() # Removed: no longer needed\n",
        "\n",
        "# instantiate CAM objects separately for the two target layers\n",
        "cam_vit = None\n",
        "cam_reg = None\n",
        "if vit_last_conv is not None:\n",
        "    cam_vit = GradCAM(model=model, target_layers=[vit_last_conv])\n",
        "if regnet_last_conv is not None:\n",
        "    cam_reg = GradCAM(model=model, target_layers=[regnet_last_conv])\n",
        "\n",
        "fig = plt.figure(figsize=(12, 3*n_examples))\n",
        "for i,(path,label) in enumerate(sample_records):\n",
        "    img_pil = Image.open(path).convert('RGB').resize((IMG_SIZE, IMG_SIZE))\n",
        "    img_t = eval_transform(img_pil).unsqueeze(0).to(device) # Ensure input is on device\n",
        "    # forward to get predicted class\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(img_t)\n",
        "        prob = torch.sigmoid(logits).cpu().item()\n",
        "        pred_label = 1 if prob>0.5 else 0\n",
        "    # compute cams\n",
        "    cams = []\n",
        "    # Use BinaryClassifierOutputTarget for binary output\n",
        "    targets = [BinaryClassifierOutputTarget(pred_label)]\n",
        "    if cam_vit is not None:\n",
        "        grayscale_cam_v = cam_vit(input_tensor=img_t, targets=targets)[0]\n",
        "        cams.append(grayscale_cam_v)\n",
        "    if cam_reg is not None:\n",
        "        grayscale_cam_r = cam_reg(input_tensor=img_t, targets=targets)[0]\n",
        "        cams.append(grayscale_cam_r)\n",
        "    if len(cams) == 0:\n",
        "        raise RuntimeError(\"No CAM layers available.\")\n",
        "    fused_cam = np.mean(cams, axis=0)  # average cams\n",
        "    # overlay\n",
        "    rgb_img = tensor_to_numpy_img(eval_transform(img_pil))  # this gives normalized->0..1 image\n",
        "    visualization = show_cam_on_image(rgb_img, fused_cam, use_rgb=True)\n",
        "    ax = fig.add_subplot(n_examples, 2, 2*i+1)\n",
        "    ax.imshow(rgb_img)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(f\"Original: label={label} \")\n",
        "    ax2 = fig.add_subplot(n_examples, 2, 2*i+2)\n",
        "    ax2.imshow(visualization)\n",
        "    ax2.axis('off')\n",
        "    ax2.set_title(f\"Pred={pred_label} prob={prob:.3f}\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# optionally save figure to Drive\n",
        "out_fig = '/content/drive/MyDrive/cbis_ddsm_gradcam_examples.png'\n",
        "fig.savefig(out_fig)\n",
        "print(\"Saved Grad-CAM image to:\", out_fig)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "MyzMorBYxLK1",
        "outputId": "4138acba-e2a7-4f97-823b-8162aefd11ff"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'test_records' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-375140898.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# pick a few examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mn_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msample_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_records\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_examples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# use_cuda = torch.cuda.is_available() # Removed: no longer needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_records' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ql4oX2DpxLNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WC-xeAydxLPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEcFe7N6xLQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "alre6Ge5xLTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tu2JrCeOxLVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsec7E4sxLXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QytU9i5XxLZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oXyDFtk0xLbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QIbqU77oxLdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pHGYaxRnxLgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AegdQPmoxLiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7TTxv-eoxLkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kqQ3_2wrxLm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gK2wGQvXxLoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "etpmW8XPxLrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8JB09UkkxLtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kjEjG5HSxLvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C7COhueLxLxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFf-NkerxLzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nri9BXtxL2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2tvcN5XBxL4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQEXLQ1nxL6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ERr-WAVqxL8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccffc94f",
        "outputId": "d62a9622-a9e8-4549-d8fb-a587a60fc382"
      },
      "source": [
        "# Helper to find last Conv2d in a given submodule (robust for timm models)\n",
        "import torch.nn as nn\n",
        "\n",
        "def find_last_conv(module):\n",
        "    last_conv = None\n",
        "    for m in module.modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            last_conv = m\n",
        "    return last_conv\n",
        "\n",
        "# find convs in both backbones inside the fusion model\n",
        "vit_last_conv = find_last_conv(model.vit)\n",
        "regnet_last_conv = find_last_conv(model.regnet)\n",
        "print(\"vit last conv:\", vit_last_conv)\n",
        "print(\"regnet last conv:\", regnet_last_conv)\n",
        "if vit_last_conv is None or regnet_last_conv is None:\n",
        "    print(\"Warning: could not find last conv in one of the backbones. Grad-CAM may fail.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vit last conv: Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "regnet last conv: Conv2d(576, 1512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f09acaa1",
        "outputId": "5e0fdc0f-e15b-49ea-dadb-c0df1ee2d535"
      },
      "source": [
        "# Cell 8: FusionModel - ViT + RegNet features concatenation\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm import create_model\n",
        "\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, vit_name='vit_base_patch16_224', regnet_name='regnety_032', pretrained=True, dropout=0.3):\n",
        "        super().__init__()\n",
        "        # create backbones with no classification head (num_classes=0)\n",
        "        self.vit = create_model(vit_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
        "        self.regnet = create_model(regnet_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
        "        self.vit_feat = getattr(self.vit, 'num_features', None)\n",
        "        self.regnet_feat = getattr(self.regnet, 'num_features', None)\n",
        "        if self.vit_feat is None or self.regnet_feat is None:\n",
        "            # fallback to attribute names sometimes used\n",
        "            self.vit_feat = getattr(self.vit, 'head', None) and getattr(self.vit.head, 'in_features', None) or 768\n",
        "            self.regnet_feat = getattr(self.regnet, 'head', None) and getattr(self.regnet.head, 'in_features', None) or 1024\n",
        "\n",
        "        fused_dim = int(self.vit_feat + self.regnet_feat)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fused_dim, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(512, 1)   # binary logit\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timm models generally have forward_features\n",
        "        v = self.vit.forward_features(x)\n",
        "        r = self.regnet.forward_features(x)\n",
        "\n",
        "        # ensure vectors (global pooling might still be present)\n",
        "        if v.dim() > 2:\n",
        "            # e.g. [B, N, C] or [B, C, H, W]\n",
        "            v = v.view(v.size(0), v.size(1), -1) if v.dim()==3 else v\n",
        "            # final reduce\n",
        "            v = v.mean(dim=1) if v.dim()==3 else F.adaptive_avg_pool2d(v, 1).reshape(v.size(0), -1)\n",
        "\n",
        "        if r.dim() > 2:\n",
        "            r = r.view(r.size(0), r.size(1), -1) if r.dim()==3 else r\n",
        "            r = r.mean(dim=1) if r.dim()==3 else F.adaptive_avg_pool2d(r, 1).reshape(r.size(0), -1)\n",
        "\n",
        "        x = torch.cat([v, r], dim=1)\n",
        "        logits = self.classifier(x).squeeze(1)\n",
        "        return logits\n",
        "\n",
        "# build model and send to device\n",
        "model = FusionModel(vit_name='vit_base_patch16_224', regnet_name='regnety_032', pretrained=True, dropout=0.3)\n",
        "model = model.to(device)\n",
        "print(\"Model built. vit_feat, regnet_feat:\", model.vit_feat, model.regnet_feat)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:timm.models._builder:Unexpected keys (norm.bias, norm.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model built. vit_feat, regnet_feat: 768 1512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16db8943",
        "outputId": "501879eb-8a84-409f-a6bb-13acc346d607"
      },
      "source": [
        "# Cell 2: imports and seeds\n",
        "import os, random, time, math\n",
        "from glob import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "\n",
        "import timm\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d0093d9",
        "outputId": "4f4c0288-7f2c-4f5e-f571-6556d5ab962c"
      },
      "source": [
        "!pip install -q pytorch-grad-cam"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-grad-cam (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-grad-cam\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af9d18b6",
        "outputId": "41033854-a2e5-4f84-b3eb-9dcbf5f85f2a"
      },
      "source": [
        "!pip install -q grad-cam"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m845.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    }
  ]
}